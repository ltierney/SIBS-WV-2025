---
title: "Basic Data Wrangling and Data Visualization in R"
author: "Luke Tierney"
institute: "University of Iowa"
date: "26 June, 2025"
output:
  xaringan::moon_reader:
    includes:
      after_body: "collapsecode.js"
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: true
      titleSlideClass: [center, middle]
---

class: center, middle

```{r setup, include = FALSE}
if ("xaringan" %in% loadedNamespaces()) {
    options(htmltools.dir.version = FALSE)
    knitr::opts_chunk$set(fig.height = 5, fig.width = 6)
    xaringanExtra::use_tile_view()
    xaringanExtra::use_clipboard()
    xaringanExtra::use_search(show_icon = TRUE)
}
knitr::opts_chunk$set(collapse = TRUE, warning = FALSE)
library(ggplot2)
theme_set(theme_minimal() +
          theme(text = element_text(size = 16)) +
          theme(panel.border = element_rect(color = "grey30", fill = NA)))
here_rel <- function(path)
    if (file.exists(path)) path else file.path("..", path)
```

```{css, echo = FALSE}
.content-box-blue { background-color: lightblue; }
.small-font { font-size: 70%; }
.width-20 { width: 20% }
.width-30 { width: 30% }
.width-60 { width: 60% }
.width-70 { width: 70% }
.note {
	padding: 15px;
	margin-bottom: 20px;
	border: 1px solid transparent;
	border-radius: 4px;
	background-color: #d9edf7;
	border-color: #bce8f1;
	color: #31708f;
}
.hljs-comment {
	color: navy !important;
}
```

# Introduction

---
## Outline

In this class I will

* Briefly outline the history of R.

--

* Using some examples briefly show how to do data wrangling
  and visualize data in R.

--

Materials for this class are available on GitHub at
<https://github.com/ltierney/SIBS-WV-2025>.

--

* You can access it as an RStudio project by following the menu selection
  **File > New Project > Version Control > Git** and specifying this URL.

--

* You can use the `git` command line client with
    ```shell
git clone https://github.com/ltierney/SIBS-WV-2025
    ```

--

Materials are also available at
<https://www.stat.uiowa.edu/~luke/SIBS-R/SIBS-WV-2025/>.

--

Materials for our _Data Visualization and Data
Technologies_ course are available at

<https://www.stat.uiowa.edu/~luke/classes/STAT4580-2025/>


---

## Tools

Some tools I will be using:

--

* The [RStudio](https://posit.co/download/rstudio-desktop/) IDE.

--

* Many features from the basic [R](https://www.r-project.org) distribution.

--

* Some tools from the [_tidyverse_](https://www.tidyverse.org/).

--

* The [`ggplot`](https://ggplot2.tidyverse.org/) package based on
  the _Grammar of Graphics_ framework.

--

Most of the packages are loaded by loading the `tidyverse` package:

```{r, message = FALSE}
library(tidyverse)
```

---
## References

Useful references:

--

> Hadley Wickham, Mine Ã‡etinkaya-Rundel, and Garrett Grolemund (2023),
> [_R for Data Science (2nd Edition)_](https://r4ds.hadley.nz/),
> O'Reilly.

--

> Claus O. Wilke (2019), [_Fundamentals of Data
>  Visualization_](https://clauswilke.com/dataviz/), O'Reilly.

--

> Kieran Healy (2018) [_Data Visualization: A practical
> introduction_](https://socviz.co/), Princeton

--

> Rafael A. Irizarry (2019), [Introduction to Data Science: _Data
> Analysis and Prediction Algorithms with
> R_](https://rafalab.dfci.harvard.edu/dsbook/), Chapman & Hall/CRC. ([Book
> source on GitHub](https://github.com/rafalab/dsbook))

--

```{r, include = FALSE}
tutorial <- here_rel("tutorial/penguins.Rmd")
```

There is also a small interactive [tutorial](`r tutorial`) available.

--

**Ask questions any time!**

---
class: center, middle

# The R Language

---
## Background

R is a language for data analysis and graphics.

--

* R was originally developed by Robert Gentleman and Ross Ihaka in the
  early 1990's for a Macintosh computer lab at U. of Auckland, New Zealand.

--

* R is based on the S language developed by John Chambers and
  others at Bell Labs.

--

R is an Open Source project.

--

* Since 1997 R is developed and maintained by the R-core group,
  with around 20 members located in more than 10 different countries.

--

* R is widely used in the field of statistics and beyond, especially in
  university environments.

--

* R has become the primary framework for developing and making available
  new statistical methodology.

--

* Many (now over 20,000) extension packages are available through CRAN or
  similar repositories.

---
layout: true
## Working with R
---

R is designed for interactive data exploration.

--

* Interaction is through a _read-eval-print loop (REPL)_.

--

* This is also called a _command line interface (CLI)_.

--

All computations are specified in the R language.

--

* Even for simple tasks you need to know a little of the language.

--

* After learning to do simple tasks you know some of the language.

--

The language is used to

--

* prepare data for analysis;

--

* specify individual analyses;

--

* program repeated or similar analyses;

--

* program new methods of analysis.

--

Specifying these tasks in a language supports _reproducible research_.

---

The R language operates on vectors and arrays.

--

Commonly used data types are:

--

* integer and numeric vectors;

--

* logical vectors;

--

* character vectors;

--

* factors.

--

All basic vector types support missing (`NA`) values.

--

Arithmetic operations are vectorized to operate element-wise on vectors.

--

Data vectors are usually combined into table-like objects called _data
frames_.


---

Some simple examples:

--

.pull-left.width-20[
Scalar arithmetic:
]
--
.pull-right.width-70[
```{r, prompt = TRUE, comment = ""}
1 + 2
```
]

--

.pull-left.width-20[
Making vectors:
]
--
.pull-right.width-70[
```{r prompt = TRUE, comment = ""}
c(2, 4, 6)
```
{{content}}
]
--
```{r prompt = TRUE, comment = ""}
1 : 4
```

--

.pull-left.width-20[
Defining variables:
]
--
.pull-right.width-70[
```{r, prompt = TRUE, comment = ""}
x <- c(2, 4, 6)
```
{{content}}
]
--
The result is returned _invisibly_.


---

.pull-left.width-20[
Vector arithmetic:
]
--
.pull-right.width-70[
```{r prompt = TRUE, comment = ""}
x + 1
```

{{content}}
]
--
```{r prompt = TRUE, comment = ""}
x + x
```

--

.pull-left.width-20[
Vectorized math functions:
]
--
.pull-right.width-70[
```{r prompt = TRUE, comment = ""}
log(x)
```
]

--

.pull-left.width-20[
Single elements:
]
--
.pull-right.width-70[
```{r prompt = TRUE, comment = ""}
x[[1]]
```
]

.pull-left.width-20[
Subsets:
]
--
.pull-right.width-70[
```{r prompt = TRUE, comment = ""}
x[1 : 2]
```
]

---

.pull-left.width-20[
Logical indexing:
]
--
.pull-right.width-70[
```{r, prompt = TRUE, comment = ""}
x[x > 2]
```
]

--

.pull-left.width-20[
Creating data frames:
]
--
.pull-right.width-70[
```{r, prompt = TRUE, comment = ""}
d <- data.frame(x, y = log(x))
d
```
]

--

.pull-left.width-20[
Accessing variables in data frames:
]
.pull-right.width-70[
```{r, prompt = TRUE, comment = ""}
d$x
```
]

---
layout: false
## The Data Analysis Process

A figure that shows the steps usually involved in a data analysis
project:

```{r, include = FALSE}
library(nomnoml)
```
<center>
```{nomnoml, echo = FALSE, fig.height = 5, fig.width = 8}
#padding: 25
#fontsize: 18
#fill: #E1DAFF; #D4A9FF
#stroke: #8515C7
#linewidth: 2

[Import] -> [Understand]
[Understand |
  [Wrangle] -> [Visualize]
  [Visualize] -> [Model]
  [Model] -> [Wrangle]
]
[Understand] -> [Communicate]
```
</center>

--

These steps are often repeated many times, so it is important to make
your work reproducible.

---
## Reproducible Data Analysis

Making your work reproducible:

--

* Save you work in a text file or notebook.

--

* Track changes to your files with a version control system like
  [`git`](https://git-scm.com/).

--

* Use a system like [Rmarkdown](https://rmarkdown.rstudio.com) to
  prepare your reports.

--

This allows you to re-create your report when data changes (as it
often will!).

--

A good resource for setting up your tools to support this is [_Happy
Git and GitHub for the useR_](https://happygitwithr.com/).


---
class: center, middle

# Some Examples

---
## Example Data Sets

When working with research data, a first step is usually to read and
clean the data.

--

We'll put that off for a little while and work with some data sets
made available in R packages.

--

Data sets available in R packages include:

--

* many classic data sets;

--

* newer, often larger, data sets useful for learning;

--

* current data obtained by querying web APIs.


---
layout: true
## Old Faithful Eruptions
---

A simple classic data set is the `geyser` data frame available in
package `MASS`.

--

.pull-left[
```{r}
data(geyser, package = "MASS")
dim(geyser)
head(geyser, 4)
```
]
--
.pull-right[
<div class = "note">
`head` and `tail` return the first and last few rows of a data frame.

They are useful for quick sanity checks.
</div>
<!-- foobar - don't ask -->
]

--

The rows represent measurements recorded for eruptions of the [_Old
Faithful_](https://www.yellowstonepark.com/things-to-do/geysers-hot-springs/about-old-faithful/)
geyser in Yellowstone National Park, Wyoming.

--

The variables are:

* `waiting`: the time in minutes since the previous eruption;

--

* `duration`: the duration in minutes of the eruption.


---

The durations have a bimodal distribution:

.pull-left[
```{r geyser-hist, echo = FALSE}
ggplot(geyser) +
    geom_histogram(aes(x = duration),
                   bins = 15,
                   color = "black",
                   fill = "grey")
```
]
--
.pull-right[
```{r geyser-hist, eval = FALSE}
```

{{content}}
]
--
This is [`ggplot`](https://ggplot2.tidyverse.org/) code for creating a
histogram.
<!-- # nolint start -->

{{content}}
--

<div class = "note">
A basic template for creating a plot with `ggplot`:

```r
ggplot(data = <DATA>) +
    <GEOM>(mapping = aes(<MAPPINGS>))
```
</div>
<!-- # nolint end -->

---

An interesting question is whether the duration can be used to predict
when the _next_ eruption will occur.

--

A plot of the _previous_ duration against the waiting time to the
current eruption:

--

.pull-left[
```{r geyser-scatter, echo = FALSE}
ggplot(geyser) +
    geom_point(aes(x = lag(duration),
                   y = waiting))
```
]
--
.pull-right[
```{r geyser-scatter, eval = FALSE}
```

{{content}}
]
--
It looks like a useful rule would be to expect a shorter waiting time
after a shorter eruption duration.


---

An interesting feature:

--

Many durations are recorded as 2 or 4 minutes.

--

This can also be seen in a histogram with a small bin width:

.pull-left[
```{r geyser-hist-narrow, echo = FALSE}
p <- ggplot(geyser) +
    geom_histogram(aes(x = duration,
                       y = stat(density)),
                   fill = "grey",
                   color = "black",
                   binwidth = 0.1) #<<
p
```
]
--
.pull-righ[
```{r geyser-hist-narrow, eval = FALSE}
```

{{content}}
]
--

<div class = "note">
`ggplot` produces a plot object.

Drawing only happens when the object is printed.
</div>
<!-- foobar - don't ask -->

---

Does this rounding matter?

--

* For many analyses it probably doesn't.

--

* It might if you wanted to fit normal distributions to the two groups.

--

.pull-left[
Taking 3 minutes as the divide between short and long durations we can
first pick out the short and long durations:

```{r}
d <- geyser$duration
d_short <- d[d < 3]
d_long <- d[d >= 3]
```
]
--
.pull-right[
Then compute the means and standard deviations as

```{r}
mean(d_short)
sd(d_short)
```
```{r}
mean(d_long)
sd(d_long)
```
```{r}
mean(d >= 3)
```
]

---

An approach that scales better:

--

Compute group summaries using tools from the `dplyr` tidyverse
package.

--

First, add a `type` variable:

```{r}
geyser <- mutate(geyser, type = ifelse(duration < 3, "short", "long"))
```

--

The summaries can then be computed as

.pull-left[
```{r}
sgd <- summarize(group_by(geyser, type),
                 mean = mean(duration),
                 sd = sd(duration),
                 n = n())
(sgd <- mutate(sgd, prop = n / sum(n)))
```
]
--
.pull-right[
<div class = "note">
The functions `summarize`, `group_by`, and `mutate` are from the
`dplyr` package that implements a _grammar of data manipulation_.
</div>
<!-- foobar - don't ask -->
]

---

.pull-left[
This computation can also be written using the _forward pipe operator_ `|>`:
{{content}}
]
--

```{r}
sgd <-
    group_by(geyser, type) |>
    summarize(mean = mean(duration),
              sd = sd(duration),
              n = n()) |>
    ungroup() |>
    mutate(prop = n / sum(n))
sgd
```

--

.pull-right[
<div class = "note">
The pipe operator allows a sequence of operations to be chained together.

{{content}}
]
--

The left-hand result is passed implicitly as the first argument to
the function called on the right.

{{content}}
--
You can also use the older `%>%` pipe operator from the `magrittr` package.
</div>
<!-- foobar - don't ask -->

---

One way to show the superimposed normal densities:

.pull-left[
```{r geyser-hist-dens, echo = FALSE}
f1 <- function(x)
    sgd$prop[1] *
        dnorm(x, sgd$mean[1], sgd$sd[1])
f2 <- function(x)
    sgd$prop[2] *
        dnorm(x, sgd$mean[2], sgd$sd[2])
p <- p +
    stat_function(color = "red", fun = f1) +
    stat_function(color = "blue", fun = f2)
p
```
]
--
.pull-right[
```{r geyser-hist-dens, eval = FALSE}
```

{{content}}
]
--

<div class = "note">
A `ggplot` can consist of several _layers_.
</div>
<!-- foobar - don't ask -->

---

The means and standard deviations are affected by the rounding.

--

Summaries that omit values equal to 2 or 4 minutes can be computed as

```{r}
sgd2 <-
    filter(geyser, duration != 2, duration != 4) |>
    group_by(type) |>
    summarize(mean = mean(duration),
              sd = sd(duration),
              n = n()) |>
    ungroup() |>
    mutate(prop = n / sum(n))
sgd2
```

---

A plot showing curves computed both ways:

--
.pull-left[
```{r geyser-hist-dens-2, echo = FALSE}
f1_2 <- function(x)
    sgd2$prop[1] *
        dnorm(x, sgd2$mean[1], sgd2$sd[1])
f2_2 <- function(x)
    sgd2$prop[2] *
        dnorm(x, sgd2$mean[2], sgd2$sd[2])
p <- p +
    stat_function(color = "red",
                  linetype = 2,
                  fun = f1_2) +
    stat_function(color = "blue",
                  linetype = 2,
                  fun = f2_2)
p
```
]
--
.pull-right[
```{r geyser-hist-dens-2, eval = FALSE}
```
]

```{r, eval = FALSE, echo = FALSE}
## Fancier version that gets a color legend.
## Could also get a line type legend.
p <- ggplot(geyser) +
    geom_histogram(aes(x = duration, y = stat(density)),
                   fill = "grey", color = "black", bins = 50)
p <- p + 
    stat_function(aes(color = type),
                  data = filter(sgd, type == "long"),
                  fun = function(x)
                          sgd$prop[1] * dnorm(x, sgd$mean[1], sgd$sd[1])) +
    stat_function(aes(color = type),
                  data = filter(sgd, type == "short"),
                  fun = function(x)
                          sgd$prop[2] * dnorm(x, sgd$mean[2], sgd$sd[2]))
p

p <- p +
     stat_function(aes(color = type),
                  data = filter(sgd2, type == "long"),
                  linetype = 2,
                  fun = function(x)
                          sgd2$prop[1] * dnorm(x, sgd2$mean[1], sgd2$sd[1])) +
    stat_function(aes(color = type),
                  data = filter(sgd2, type == "short"),
                  linetype = 2,
                  fun = function(x)
                          sgd2$prop[2] * dnorm(x, sgd2$mean[2], sgd2$sd[2]))
p
```

---
layout: true
## Minnesota Barley Yields
---

Another classic data set:

--

Total yield in bushels per acre for 10 varieties at 6 sites in
Minnesota in each of two years, 1931 and 1932.

--

The raw data:

```{r}
data(barley, package = "lattice")
head(barley)
```

---

Some initial plots:

```{r, fig.width = 10}
p1 <- ggplot(barley) + geom_point(aes(x = yield, y = variety))
p2 <- ggplot(barley) + geom_point(aes(x = yield, y = site))
library(patchwork)
p1 + p2
```

---

Using color to separate yields in the two years:

```{r, fig.width = 12}
p1 <- ggplot(barley) + geom_point(aes(x = yield, y = variety, color = year))
p2 <- ggplot(barley) + geom_point(aes(x = yield, y = site, color = year))
p1 + p2

```

---

Can we also show `site` using symbol shape?

--

.pull-left[
```{r barley-color-sym, echo = FALSE, fig.width = 7}
ggplot(barley) +
    geom_point(aes(x = yield,
                   y = variety,
                   color = year,
                   shape = site)) #<<
```
]
--
.pull-right[
```{r barley-color-sym, eval = FALSE}
```

{{content}}
]
--
There is a lot of _interference_ between shape and color.


---

Larger points may help:

.pull-left[
```{r barley-color-sym-2, echo = FALSE, fig.width = 7}
ggplot(barley) +
    geom_point(aes(x = yield,
                   y = variety,
                   color = year,
                   shape = site),
               size = 2.5) #<<
```
]
.pull-right[
```{r barley-color-sym-2, eval = FALSE}
```
]


---

_Jittering_ may also help:


.pull-left[
```{r barley-color-sym-3, echo = FALSE, fig.width = 7}
ggplot(barley) +
    geom_point(aes(x = yield,
                   y = variety,
                   color = year,
                   shape = site),
               size = 2.5,
               position =             #<<
                   position_jitter(   #<<
                       height = 0.15, #<<
                       width = 0))    #<<
```
]
.pull-right[
```{r barley-color-sym-3, eval = FALSE}
```
]


---

.pull-left[
Another approach: _faceting_ to produce _small multiples_.

```{r barley-facet, eval = FALSE}
ggplot(barley) +
    geom_point(aes(x = yield,
                   y = variety,
                   color = year)) +
    facet_wrap(~site, ncol = 2)    #<<
```
]
.pull-right[
```{r barley-facet, echo = FALSE, fig.width = 7, fig.height = 7}
```
]

---

Focusing on summaries can help.

--
A _dot plot_ of average yields for each site and year:

.pull-left[
```{r barley-avg-dot, echo = FALSE, message = FALSE, fig.width = 7}
barley_site_year <-
    group_by(barley, site, year) |>
    summarize(yield = mean(yield)) |>
    ungroup()

ggplot(barley_site_year) +
    geom_point(aes(y = site,
                   x = yield,
                   color = year),
               size = 3)
```
]
--
.pull-right[
```{r barley-avg-dot, eval = FALSE}
```
]

---

Adding lines can help comparing the changes. This is sometimes called
a _dumbbell chart_:

.pull-left[
```{r barley-avg-dot-2, echo = FALSE, message = FALSE, fig.width = 7}
barley_site_year <-
    group_by(barley, site, year) |>
    summarize(yield = mean(yield)) |>
    ungroup()

ggplot(barley_site_year) +
    geom_line(aes(y = site,        #<<
                  x = yield,       #<<
                  group = site),   #<<
              color = "darkgrey",  #<<
              size = 2) +          #<<
    geom_point(aes(y = site,
                   x = yield,
                   color = year),
               size = 4)           #<<
```
]
.pull-right[
```{r barley-avg-dot-2, eval = FALSE}
```
]

---

Another useful approach for showing repeated measurements is a _slope
graph_:

.pull-left[
.hide-code[
```{r}
library(ggrepel)
barley_site_year <-
    mutate(barley_site_year, year = fct_rev(year))
barley_site_year_1932 <-
    filter(barley_site_year, year == "1932")
ggplot(barley_site_year,
       aes(x = year, y = yield, group = site)) +
    geom_line() +
    geom_text_repel(aes(label = site),
                    data = barley_site_year_1932,
                    hjust = "left",
                    direction = "y") +
    scale_x_discrete(expand = expansion(mult = c(0.1, .25)),
                     position = "top") +
    labs(x = NULL, y = "Average Yield")
```
]
]
--

.pull-right[
This emphasizes the reversal for Morris.
]

---

_Bar charts_ are sometimes used for summaries, but dot plots are
usually a better choice.

.pull-left[
```{r barley-avg-bar, echo = FALSE, message = FALSE, fig.width = 7}
ggplot(barley_site_year) +
    geom_col(aes(x = yield,
                 y = site,
                 fill = year),
             size = 3,
             position = "dodge",
             width = .4)
```
]
.pull-right[
```{r barley-avg-bar, eval = FALSE}
```
]

---
layout: false
## Bar Charts and the Zero Base Line

Because of the way we perceive bars, it is important to use a [zero
base line for bar
charts](https://flowingdata.com/2015/08/31/bar-chart-baselines-start-at-zero/).

--

![](`r here_rel("img/viz3-520x294.jpg")`)
--
![](`r here_rel("img/viz5-520x280.jpg")`)

---
layout: true
## Hair and Eye Color Data
---

A data set recording the distribution of hair and eye color and sex in
592 statistics students.


.pull-left[
```{r}
HairEyeDF <- as.data.frame(HairEyeColor)
head(HairEyeDF)
```
]
--
.pull-right[
The data set is available as a _cross-tabulation_.

{{content}}
]
--
`as.data.frame` converts it to a data frame.

---

Looking at the distribution of eye color:

--
.pull-left[
```{r eye-bar, echo = FALSE}
eye <-
    group_by(HairEyeDF, Eye) |>
    summarize(Freq = sum(Freq)) |>
    ungroup()

ggplot(eye) +
    geom_col(aes(x = Eye,
                 y = Freq),
             position = "dodge")
```
]
--
.pull-right[
```{r eye-bar, eval = FALSE}
```
]

---

Mapping eye color to bar color in addition to the horizontal axis
position can help:

.pull-left[
```{r eye-bar-2, echo = FALSE}
eye <-
    group_by(HairEyeDF, Eye) |>
    summarize(Freq = sum(Freq)) |>
    ungroup()

ggplot(eye) +
    geom_col(aes(x = Eye,
                 y = Freq,
                 fill = Eye), #<<
             position = "dodge")
```
]
.pull-right[
```{r eye-bar-2, eval = FALSE}
```
]

---

More sensible colors would be nice but require a bit of work:

.pull-left[
```{r eye-bar-3, echo = FALSE}
hazel_rgb <-
    col2rgb("brown") * 0.75 + col2rgb("green") * 0.25
hazel <-
    do.call(rgb, as.list(hazel_rgb / 255))

cols <-
    c(Blue = colorspace::lighten(colorspace::desaturate("blue", 0.3), 0.3),
      Green = colorspace::lighten("forestgreen", 0.1),
      Brown = colorspace::lighten("brown", 0.0001), ## 0.3?
      Hazel = colorspace::lighten(hazel, 0.3))

pb <- ggplot(eye) +
    geom_col(aes(x = Eye,
                 y = Freq,
                 fill = Eye),
             position = "dodge") +
    scale_fill_manual(values = cols)
pb
```
]
.pull-right[
```{r eye-bar-3, eval = FALSE}
```
]

---

A _stacked bar chart_ can also be useful:

.pull-left[
```{r eye-bar-stacked, echo = FALSE}
psb <- ggplot(eye) +
    geom_col(aes(x = "",
                 y = Freq,
                 fill = Eye),
             color = "lightgrey") +
    scale_fill_manual(values = cols)
psb
```
]
.pull-right[
```{r eye-bar-stacked, eval = FALSE}
```
]

---

Changing to polar coordinates produces a _pie chart_:

.pull-left[
```{r eye-pie, echo = FALSE}
(pp <- psb + coord_polar("y"))
```
]
.pull-right[
```{r eye-pie, eval = FALSE}
```
]

---

The axis and grid are not helpful; a _theme_ adjustment can remove them:

.pull-left[
```{r eye-pie-2, echo = FALSE}
(pp <- pp + theme_void())
```
]
.pull-right[
```{r eye-pie-2, eval = FALSE}
```

{{content}}
]
--
<div class = "note">
Themes provide a way to customize the non-data components of plots:
i.e. titles, labels, fonts, background, grid lines, and legends.

{{content}}
--
Themes can be used to give plots a consistent customized look.

{{content}}
--
The `ggthemes` package provides a number of themes to emulate the
style of different publications, for example `theme_wsj` and
`theme_economist`.
</div>
<!-- foobar - don't ask -->

---

How well do bar charts and pie charts work?

--

.pull-left[
```{r, echo = FALSE, fig.height = 4, fig.width = 8}
cowplot::plot_grid(pb, pp)
```
]

--

.pull-right[
Some questions:

{{content}}
]
--
* Which plot makes it easier to tell whether the proportion of
  brown-eyed students is larger or smaller than the proportion of
  blue-eyed students?

{{content}}
--
* Which plot makes it easier to tell whether these proportions are
  larger or smaller than 1/2 or 1/4 or 1/3?

---

Looking at the proportions within hair color and sex:

.hide-code[
```{r, fig.width = 14, fig.height = 6}
eye_hairsex <-
    group_by(HairEyeDF, Hair, Sex) |>
    mutate(Prop = Freq / sum(Freq)) |>
    ungroup()

p1 <- ggplot(eye_hairsex) +
    geom_col(aes(x = Eye, y = Prop, fill = Eye)) +
    scale_fill_manual(values = cols) +
    facet_grid(Hair ~ Sex)
p2 <- ggplot(eye_hairsex) +
    geom_col(aes(x = "", y = Prop, fill = Eye)) +
    scale_fill_manual(values = cols) +
    coord_polar("y") +
    facet_grid(Hair ~ Sex) +
    theme_void()
cowplot::plot_grid(p1, p2)
```
]

---
layout: false
## A more complete `ggplot` template

<!-- # nolint start -->

```r
ggplot(data = <DATA>) +
    <GEOM>(mapping = aes(<MAPPINGS>),
           stat = <STAT>,
           position = <POSITION>) +
    < ... MORE GEOMS ... > +
    <COORDINATE_ADJUSTMENT> +
    <SCALE_ADJUSTMENT> +
    <FACETING> +
    <THEME_ADJUSTMENT>
```
<!-- # nolint end -->


---
class: center, middle
# Visual Perception and the Grammar of Graphics

---
layout: true

## Monthly River Flows

Monthly flow volumes recorded for a river in the pacific north-west.

---

An initial plot using default settings:

.hide-code[
```{r, fig.width = 8}
library(ggplot2)
river <- scan(here::here("data/river.dat"))
rd <- data.frame(flow = river, month = seq_along(river))
(pp <- ggplot(rd) + geom_point(aes(x = month, y = flow)))
```
]

---

Changing the _aspect ratio_:

.hide-code[
```{r, fig.width = 12, fig.height = 4}
pp + coord_fixed(3.5)
```
]

---

Time series are often visualized with a line plot:

.hide-code[
```{r, fig.width = 12, fig.height = 4}
pl <- ggplot(rd) + geom_line(aes(x = month, y = flow))
pl + coord_fixed(3.5)
```
]

---

The seasonal variation can be seen with a line plot in the original
aspect ratio:

.hide-code[
```{r, fig.width = 8}
pl
```
]

---
layout: true
## A Simple Model of Visual Perception
---

The eyes acquire an image, which is processed through three stages of
memory:

--

* Iconic memory

--

* Working memory, or short-term memory

--

* Long-term memory

--

The first processing stage of an image happens in iconic memory.

--

* Images remain in iconic memory for less than a second.

--

* Processing in iconic memory is massively parallel and automatic.

--

* This is called _preattentive processing_.

--

Preattentive processing is a fast recognition process.

---

Meaningful visual chunks are moved from iconic memory to short term memory.

--

* These chunks are used by conscious, or attentive, processing.

--

* Attentive processing often involves conscious comparisons or search.

--

* Short term memory is limited;

    * information is retained for only a few seconds;
    * only three or fours chunks can be held at a time.

--

Long term visual memory is built up over a lifetime, though
infrequently used visual chunks may become lost.

--
<div class = "note">
**Visual Design Implications**

Try to make as much use of preattentive features as possible.

--

Recognize when preattentive features might mislead.

--

For features that require attentive processing, keep in mind that
working memory is limited.
</div>
<!-- foobar - don't ask -->


---
layout: false
## Some Terms for Describing Visualizations

Data to be visualized contains _variables_ or _attributes_ measured on
individual _items_ or _cases_.

--

_Links_ are relationships that may exist among items, e.g. months
within a year or countries within a continent.

--

_Marks_ are individual geometric entities used to represent items:
points, bars, etc.

--

_Aesthetics_ or _visual channels_ are the visual features of marks
that can be used to encode attributes.

--

The `aes(...)` expressions establish the mapping between attributes
and visual channels.

--

These ideas closely mirror the structure of the _grammar of graphics_
as implemented in `ggplot`.

--

> Munzner, T. (2014), [_Visualization Analysis and
>  Design_](https://www.cs.ubc.ca/~tmm/vadbook/), CRC Press.

> Wilkinson, L. (2005), _The Grammar of Graphics_, 2nd ed, Springer.


---
layout: true
## Channels and their Accuracy
---

A useful distinction among channels:

--

* _Magnitude channels_ can reflect order and numeric values,
  e.g. position on an axis, length, area, brightness.

--

* _Identity channels_ can distinguish different values but not reflect
  order, e.g. hue, shape, grouping.

--

Some channels are better at conveying information than others.

---

Munzner's ordering by accuracy:

--
.small-font[

| Magnitude Channels (Ordered, Numerical) | Identity Channels (Categorical) |
|-----------------------------------------|---------------------------------|
| Position on common scale                | Spatial grouping                |
| Position on unaligned scale             | Color hue                       |
| Length (1D size)                        | Shape                           |
| Tilt, angle                             |                                 |
| Area (2D size)                          |                                 |
| Depth (3D position)                     |                                 |
| Color luminance, saturation             |                                 |
| Curvature, volume (3D size)             |                                 |

]

--

Line width is another channel; not sure there is agreement on its
accuracy, but it is not high.

--
<div class = "note">
**Visual Design Implications**

Try to map the most important variables to the strongest channels.
</div>
<!-- foobar - don't ask -->

---
layout: true
## Color
---

Color is very effective when used well.

--

But using color well is not easy.

--

Some of the issues:

--

* Perception depends on context.

--

* Simple color assignments may not separate equally well.

--

* Effectiveness may vary with the medium (screen, projector, print).

--

* Some people do not perceive the full specturm of colors.

--

* Grey scale printing.

--

* Some colors have cultural significance.

--

* Cultural significance may vary among cultures and with time.

---

Color perception is relative:

--

![](`r here_rel("img/chess1.png")`)
--
![](`r here_rel("img/chess2.png")`)


---

Groups of colors that work well together are called _palettes_.

--

Some tools for selecting palettes include:

--

* [ColorBrewer](https://colorbrewer2.org); available in the
  `RColorBrewer` package.

--

* [HCL Wizard](https://hclwizard.org/); also available as `hclwizard`
  in the `colorspace` package.

--

A note on [rainbow colors](https://www.zeileis.org/news/endrainbow/).

---
layout: false
class: center, middle

# A Grammar of Data Manipulation

---
layout: true
## The `dplyr` Package
---

The `dplyr` package provides a language, or grammar, for data
manipulation.

--

The design of `dplyr` is strongly motivated by SQL.

--

The language contains a number of _verbs_ that operate on tables.

--

The most commonly used verbs operate on a single data frame:

--

* `select`: pick variables by their names

--

* `filter`: choose rows that satisfy some criteria

--

* `mutate`: create transformed or derived variables

--

* `arrange`: reorder the rows

--

* `summarize`: collapse rows down to summaries

---

There are also a number of `join` verbs that merge several data frames
into one.

--

Package `tidyr` provides more verbs, such as `pivot_longer` and
`pivot_wider` for reshaping data frames.

--

The single table verbs can also be used with `group_by` to work
separately on groups of rows.

---
layout: false
class: center, middle

# More Examples


---
## Raw Data

These examples start with raw data as you might receive it from a
researcher, and involve reading and cleaning the data.

--

Common data formats you might encounter include:

--

* [_CSV_ (comma-separated
  values)](https://en.wikipedia.org/wiki/Comma-separated_values) files.

--

* Text files using other delimiters, such as tabs or `|` characters.

--

* [JSON(JavaScript Object
  Notation)](https://en.wikipedia.org/wiki/JSON) files.
  
--

* [XML (Extensible Markup
  Language)](https://en.wikipedia.org/wiki/XML) files.

--

* Excel spreadsheets.

--

Tools are available for reading data in these formats into R.

---
layout: true
## Wind Turbines in Iowa
---

There are many wind turbines in Iowa.

--

Data is available from the [U.S. Wind Turbine
Database](https://eerscmap.usgs.gov/uswtdb/).

--

A snapshot is available [here](`r here_rel("data/us_wind.csv")`)
as a CSV file.

--

* CSV files are a common form of data exchange.

--

* They are simple text files that are intended to be written and read
  by a computer.

--

* Some CSV files include a header and a footer that need to he handled.

--

* One issue is that a comma isn't a good separator in countries where
  it is the decimal separator!

--

* A CSV file can be read using `read.csv` or `readr::read_csv`.

--

Reading the wind turbine data:

```{r}
wind_turbines <- read.csv(here::here("data/us_wind.csv"), comment = "#")
```

---

Some data cleaning is needed.

--

Focus on the wind turbines in IOWA (19 is the state component of the
[FIPS county code](https://en.wikipedia.org/wiki/FIPS_county_code) for
Iowa):

```{r}
wt_IA <- filter(wind_turbines, t_fips %/% 1000 == 19)
```
--

Drop entries with missing longitude or latitude values:

```{r}
wt_IA <- filter(wt_IA, ! is.na(xlong), ! is.na(ylat))
```
--

Some missing year values are encoded as -9999; replace these with `NA`:

```{r}
wt_IA <- mutate(wt_IA, p_year = replace(p_year, p_year < 0, NA))
```

---

To show the locations of wind turbines on a map, load some map data:

.pull-left[
```{r iowa_sf_map, eval = FALSE}
iowa_sf <-
    sf::st_as_sf(maps::map("county", "iowa",
                           plot = FALSE,
                           fill = TRUE))

p <- ggplot() +
    geom_sf(data = iowa_sf) +
    ggthemes::theme_map()
p
```
]

.pull-right[
```{r iowa_sf_map, echo = FALSE, fig.width = 8}
```
]

---

Locations for all wind turbines in iowa:

.pull-left[
```{r wt-IA-all, eval = FALSE}
p + geom_point(aes(xlong, ylat),
               data = wt_IA)
```
]
.pull-right[
```{r wt-IA-all, echo = FALSE, fig.width = 8}
```
]

---

Using color to show when the wind turbines were  built:

.pull-left[
```{r wt-IA-color, eval = FALSE}
year_brk <- c(0, 2005, 2010, 2015, 2020, Inf)
year_lab <- c("before 2005",
              "2005-2009",
              "2010-2014",
              "2015-2020",
              "2021 and later")
wt_IA <-
    mutate(wt_IA,
           year = cut(p_year,
                      breaks = year_brk,
                      labels = year_lab,
                      right = FALSE))
p + geom_point(aes(xlong,
                   ylat,
                   color = year),
               data = wt_IA,
               size = 3)
```
]
.pull-right[
```{r wt-IA-color, echo = FALSE, fig.width = 8}
```
]


```{r eval = FALSE, echo = FALSE}
library(tidyverse)
p <- ggplot() + geom_sf(data = iowa_sf) + ggthemes::theme_map()
p + geom_point(aes(xlong, ylat), data = wt_IA)

wt_IA_sf <- sf::st_as_sf(wt_IA, coords = c("xlong", "ylat"), crs = 4326)

p + geom_sf(data = filter(wt_IA_sf, year <= 2020))

library(gganimate)
pa <- p + geom_sf(data = wt_IA_sf) +
    transition_manual(year, cumulative = TRUE) +
    labs(title = "Wind turbines in Iowa",
         subtitle = "Year = {current_frame}")
anim_save("foo.gif", animate(pa, fps = 10, nframes = 100))
```

---
layout: true
## Cancer Map
---

The website <https://www.cancer-rates.info/ia> provides data on
cancer incidence for a number of different cancers in Iowa.

--

<!-- # nolint start -->

```{r, include = FALSE}
cancer_data_file <- here_rel("data/Invasive-Cancer-Incidence-Rates-by-County-in-Iowa-Lung-and-Bronchus-2011.csv")
```

<!-- # nolint end -->

The data for lung and bronchus cancer in 2011 are available in a [csv
file](`r cancer_data_file`) in the project.

--

We can read the file with `read_csv` from the `readr` package.

--

Looking at the file shows some things that need to be cleaned up:

--

* Two header lines at the beginning.

--

* Some footer lines.

--

* Some values codes as `~`.


---

The header can be handled by using `skip = 2` in the `read_csv` call:

<!-- # nolint start -->

```{r, message = FALSE}
fname <- here::here("data/Invasive-Cancer-Incidence-Rates-by-County-in-Iowa-Lung-and-Bronchus-2011.csv")
d <- read_csv(fname, skip = 2)
head(d)
```

<!-- # nolint end -->

---

Let's focus on a few variables and give them more convenient names:

```{r}
d <- select(d, county = 1, population = 2, count = 3)
```

--

The footer needs to be removed:

```{r}
tail(d)
```

--

One way to remove the footer:

```{r}
d <- filter(d, ! is.na(population))
d <- filter(d, county != "STATE")
```

---

Changing `count` to numeric changes the `~` entries to missing values
(`NA`) values:

```{r}
d <- mutate(d, count = as.numeric(count))
```

--

In this case there are no zero counts values; two ways to check:

.pull-left[
```{r}
count(d, count == 0)
```
]
--
.pull-right[
```{r}
any(d$count == 0, na.rm = TRUE)
```
]
--
.pull-left[
It _might_ be reasonable to assume these `~` values were zeros, so replace
them with zeros:
]
--
.pull-right[
```{r}
d <- replace_na(d, list(count = 0))
```
]

---

A _choropleth map_ uses color or shading to represent values measured
for different geographic regions.

--

We will need to merge, or _left join_, the cancer data with the map
data we loaded for the wind turbine map.

--

This requires a _key_ on which to match the records in the cancer data
and the map data.

--

For Iowa this can be done with the county name, but some care is needed:

```{r}
d$county[1]
iowa_sf$ID[1]
```

--

Fixing case differences and dropping the `iowa,` prefix:


```{r}
d <- mutate(d, cname = county, county = tolower(county))
iowa_sf <- mutate(iowa_sf, ID = sub("iowa,", "", ID))
iowa_sf <- rename(iowa_sf, county = ID)
```

---

Still not quite there:

```{r}
setdiff(d$county, iowa_sf$county)
setdiff(iowa_sf$county, d$county)
```

Drop the apostrophe in O'Brien:

```{r}
d <- mutate(d, county = sub("'", "", county))

setdiff(d$county, iowa_sf$county)
setdiff(iowa_sf$county, d$county)
```


---

Define `rate1K` variable as the number of cases per 1000 inhabitants
and left join the cancer data to the map data:

```{r}
d <- mutate(d, rate1K = 1000 * (count / population))
md <- left_join(iowa_sf, d, "county")
head(md)
```

---

A simple map:

.pull-left[
```{r cancer-map-1, eval = FALSE}
ggplot(md) +
    geom_sf(aes(fill = rate1K))
```
]
--
.pull-right[
```{r cancer-map-1, echo = FALSE, fig.width = 8}
```
]

---

An improved version:

.pull-left[
```{r cancer-map-2, eval = FALSE}
library(ggthemes)
library(viridis)
ggplot(md) +
    geom_sf(aes(fill = rate1K),
            color = "grey") +
    scale_fill_viridis(
        name = "Rate per 1000") +
    theme_map()
```
]

.pull-right[
```{r cancer-map-2, echo = FALSE, fig.width = 8, message = FALSE}
```
]

---

A simple interactive version using [`plotly`](https://plotly.com/r/):

.pull-left[
```{r cancer-map-plotly, eval = FALSE}
mdl <- mutate(md,
              label = paste(cname,
                            round(rate1K, 1),
                            population,
                            sep = "\n"))
p <- ggplot(mdl) +
    geom_sf(aes(fill = rate1K,
                text = label),
            color = "grey") +
    scale_fill_viridis(
        name = "Rate per 1000") +
    theme_map()

plotly::ggplotly(p, tooltip = "text")
```
]

.pull-right[
```{r cancer-map-plotly, echo = FALSE, fig.width = 8}
```
]

---

The [`leaflet`](https://rstudio.github.io/leaflet/) package supports
more sophisticated interactive maps:

<!-- https://rstudio.github.io/leaflet/legends.html-->
.hide-code[
```{r, fig.height = 6.5, fig.width = 9}
library(leaflet)
pal <- colorNumeric(palette = "viridis", domain = md$rate1K)
lab <- lapply(paste0(md$cname, "<BR>",
                     "Rate: ", round(md$rate1K, 1), "<BR>",
                     "Pop: ", scales::comma(md$population,
                                            accuracy = 1)),
              htmltools::HTML)
leaflet(sf::st_transform(md, 4326)) |>
    addPolygons(weight = 2,
                color = "grey",
                fillColor = ~ pal(rate1K),
                fillOpacity = 1,
                highlightOptions = highlightOptions(color = "white",
                                                    weight = 2,
                                                    bringToFront = TRUE),
                label = lab) |>
    addLegend(pal = pal, values = ~ rate1K, opacity = 1)
```
]


---
layout: true
## Unemployment Map
---

[Local Area Unemployment Statistics page](https://www.bls.gov/lau/)
from the Bureau of Labor Statistics makes available county-level
monthly unemployment data for a 14-month window.

--

The file for February 2020 through March 2021 is available is
available at
<https://www.stat.uiowa.edu/~luke/data/laus/laucntycur14-2020.txt> and
in the project data folder.

--

<div class = "note">
This file is a text file but uses a non-standard separator.

--

It is designed for human readability and uses
a comma as a _thousands separator_ or _grouping mark_.

--

It also includes header and footer information.

--

It is still reasonably easy to read in.
</div>
<!-- foobar - don't ask -->

---

One way to read the data into R is:

```{r}
lausURL <- here::here("data/laucntycur14-2020.txt")
lausUS <- read.table(lausURL,
                     col.names = c("LAUSAreaCode", "State", "County",
                                   "Title", "Period",
                                   "LaborForce", "Employed",
                                   "Unemployed", "UnempRate"),
                     quote = '"', sep = "|", skip = 6,
                     stringsAsFactors = FALSE, strip.white = TRUE,
                     fill = TRUE)
footstart <- grep("------", lausUS$LAUSAreaCode)
lausUS <- lausUS[1 : (footstart - 1), ]
```

---

It may be useful to be able to access the county name and state name
separately:

```{r}
lausUS <- separate(lausUS, Title, c("cname", "scode"),
                   sep = ", ", fill = "right")
```

--

Check the variable types:

```{r}
sapply(lausUS, class)
```

--

The `UnempRate` variable is read as character data because of missing
value encoding, so needs to be converted to numeric:

```{r}
lausUS <- mutate(lausUS, UnempRate = as.numeric(UnempRate))
```

---

Check for missing values:

```{r}
select_if(lausUS, anyNA) |> names()
```

--

The state code is missing for the District of Columbia:

```{r}
select(lausUS, cname, scode) |>
    filter(is.na(scode)) |>
    unique()
```
--

<!--
Missing values for `UnempRate` are all for Puerto Rico and September
2017. Hurricane Maria made landfall on September 20. -->

March and April 2020 numbers were not available for Puerto Rico:

```{r}
select(lausUS, scode, Period, UnempRate) |>
    filter(is.na(UnempRate)) |>
    unique()
```

---

To compute the national monthly unemployment rates over this period we
need some more data cleaning:

```{r}
lausUS <- mutate(lausUS,
                 Period = fct_inorder(Period),
                 LaborForce = as.numeric(gsub(",", "", LaborForce)),
                 Unemployed = as.numeric(gsub(",", "", Unemployed)))
```

---

Unemployment during this period was affected significantly by the
COVID-19 pandemic.

A plot shows a large spike in April 2020:

.hide-code[
```{r, fig.width = 10}
group_by(lausUS, Period) |>
    summarize(Unemployed = sum(Unemployed, na.rm = TRUE),
              LaborForce = sum(LaborForce, na.rm = TRUE),
              UnempRate = 100 * (Unemployed / LaborForce)) |>
    ggplot(aes(Period, UnempRate, group = 1)) +
    geom_line()
```
]

---

A choropleth map can be used to look at how the impact was distributed
across the country.

--

To show unemployment rates on a map we need to merge the unemployment data
with map data.

--

To match county unemployment data and county shape data it is safer to
use the numeric [FIPS county
code](https://en.wikipedia.org/wiki/FIPS_county_code). This can be
added with

```{r}
lausUS <- mutate(lausUS, fips = State * 1000 + County)
```
--

Shape data for US counties can be obtained from a number of sources in a
number of different formats.

--

Here is one approach:

```{r}
counties_sf <- sf::st_as_sf(maps::map("county", plot = FALSE, fill = TRUE))
county.fips <-
    mutate(maps::county.fips, polyname = sub(":.*", "", polyname)) |>
    unique()
counties_sf <- left_join(counties_sf, county.fips, c("ID" = "polyname"))
states_sf <- sf::st_as_sf(maps::map("state", plot = FALSE, fill = TRUE))
```

---

Some summaries over the period can be computed as

```{r}
summaryUS <- group_by(lausUS, County, State, fips) |>
    summarize(avg_unemp = mean(UnempRate, na.rm = TRUE),
              max_unemp = max(UnempRate, na.rm = TRUE),
              apr_unemp = UnempRate[Period == "Apr-20"]) |>
    ungroup()
head(summaryUS)
```

---

A choropleth map of the April 2020 unemployment rates:

.hide-code[
```{r, fig.width = 9, fig.height = 6}
left_join(counties_sf, summaryUS, "fips") |>
    ggplot() +
    geom_sf(aes(fill = apr_unemp)) +
    scale_fill_viridis(name = "Rate", na.value = "red") +
    theme_map() +
    geom_sf(data = states_sf, col = "grey", fill = NA)
```
]

---

Using a very visible color for missing data is useful, at least during
exploration.

--

`anti_join` can show the county geometry that does not have an entry
in the unemployment data:

```{r}
anti_join(counties_sf, summaryUS, "fips")
```

--

Shannon County, SD (FIPS 46113), was renamed to [Oglala Lakota County](https://en.wikipedia.org/wiki/Oglala_Lakota_County,_South_Dakota) in June 2015 and given a new FIPS code, 46102.

--

The geometry data table needs to be updated:

```{r, eval = FALSE, echo = FALSE}
## old version
counties_sf <- mutate(counties_sf, fips = replace(fips, fips == 46113, 46102))
```

```{r}
counties_sf <- mutate(counties_sf,
                      fips = replace(fips, grepl("oglala", ID), 46102))
```

---

With the updated data the map is now complete:

.hide-code[
```{r, fig.width = 9, fig.height = 6}
left_join(counties_sf, summaryUS, "fips") |>
    ggplot() +
    geom_sf(aes(fill = apr_unemp)) +
    scale_fill_viridis(name = "Rate", na.value = "red") +
    theme_map() +
    geom_sf(data = states_sf, col = "grey", fill = NA)
```
]

```{r, echo = FALSE, eval = FALSE}
ggpoly2sf <- function(poly, coords = c("long", "lat"),
                      id = "group", region = "region", crs = 4326) {
    sf::st_as_sf(poly, coords = coords, crs = crs) |>
    group_by(!! as.name(id), !! as.name(region)) |>
    summarize(do_union = FALSE) |>
    sf::st_cast("POLYGON") |>
    ungroup() |>
    group_by(!! as.name(region)) |>
    summarize(do_union = FALSE) |>
    ungroup()
}
m_sf <- ggpoly2sf(socviz::county_map, c("long", "lat"), "group", "id")
m_sf <- mutate(m_sf, fips = as.numeric(id))
m_sf <- mutate(m_sf, fips = replace(fips, fips == 46113, 46102))
ggplot(m_sf) + geom_sf()
au <- group_by(lausUS, fips) |>
    summarize(avg_ur = mean(UnempRate, na.rm = TRUE))
mu <- group_by(lausUS, fips) |>
    summarize(max_ur = max(UnempRate, na.rm = TRUE))
da <- left_join(m_sf, au, "fips")
dm <- left_join(m_sf, mu, "fips")
ggplot(da, aes(fill = avg_ur)) +
    geom_sf(size = 0.1) +
    scale_fill_viridis(name = "Rate", na.value = "red")
ggplot(dm, aes(fill = max_ur)) +
    geom_sf(size = 0.1) +
    scale_fill_viridis(name = "Rate", na.value = "red")
ggplot(left_join(m_sf, filter(lausUS, Period == "Apr-20"), "fips"),
       aes(fill = UnempRate)) +
    geom_sf(size = 0.1) +
    scale_fill_viridis(name = "Rate", na.value = "red")
```
---
layout: true
## Gapminder Childhood Mortality Data
---

The `gapminder` package provides a subset of the data from the
[Gapminder](https://www.gapminder.org/) web site.

--

Additional data sets are [available](https://www.gapminder.org/data/).

--

* A data set on childhood mortality is available locally as a [csv
  file](https://homepage.stat.uiowa.edu/~luke/data/gapminder-under5mortality.csv)
  or an [Excel
  file](https://homepage.stat.uiowa.edu/~luke/data/gapminder-under5mortality.xlsx).

--

* The Excel file is also available in the project data folder.

--

* The numbers represent number of deaths within the first five years
  per 1000 births.

--

<div class="note">
Many researchers like to manage their data in a spreadsheet.

--

* Being able to read such a sheet directly greatly helps keeping the
  workflow reproducible.

--

* Many spreadsheets contain header, footers, and other annotations to
  aid a human viewer.

--

* As long as the data are in a rectangular region it is usually not
  hard to extract them programmatically.
</div>
<!-- foobar - don't ask -->


---

Loading the data:

```{r}
library(readxl)
gcm <- read_excel(here::here("data/gapminder-under5mortality.xlsx"))
```

--
A first look:

```{r}
head(gcm, 3)
```

---

This data set is in _wide_ format, with one column per year.

--

A _long_ version with a year and a value column is useful for working
with `ggplot`.

--

A better first variable name:

```{r}
names(gcm)[1] <- "country"
```

--

Convert to long format:

```{r}
tgcm <-
    pivot_longer(gcm, -1, names_to = "year", values_to = "u5mort") |>
    mutate(year = as.numeric(year))
head(tgcm, 3)
```

---

Some explorations:

.pull-left[
```{r u5-1, eval = FALSE}
p <- ggplot(tgcm) +
    geom_line(aes(year,
                  u5mort,
                  group = country),
              alpha = 0.3)
plotly::ggplotly(p)
```
]
.pull-right[
```{r u5-1, echo = FALSE, fig.height = 6, fig.width = 8}
```
]

---

Some selected countries:

.pull-left[
```{r u5-2, eval = FALSE}
countries <- c("United States",
               "United Kingdom",
               "Germany",
               "China",
               "Egypt")
filter(tgcm, country %in% countries) |>
    ggplot() +
    geom_line(aes(x = year,
                  y = u5mort,
                  color = country))
```
]
.pull-right[
```{r u5-2, echo = FALSE, fig.height = 6, fig.width = 8}
```
]

---

Examining the missing values:

.pull-left[
```{r u5-3, eval = FALSE}
tgcm_miss <-
    group_by(tgcm, country) |>
    summarize(anyNA = anyNA(u5mort)) |>
    filter(anyNA) |>
    pull(country)

p <- filter(tgcm,
            country %in% tgcm_miss) |>
    ggplot(aes(x = year,
               y = u5mort,
               group = country)) +
    geom_line(na.rm = TRUE) +
    xlim(c(1940, 2020))
plotly::ggplotly(p)
```
]
.pull-right[
```{r u5-3, echo = FALSE, fig.height = 6, fig.width = 7}
```
]

---
layout: false
class: center, middle

# Wrapping Up

---

## Final Notes

In this class I gave

--

* a brief outline of R and its history;

--

* some examples of using R to do data wrangling;

--

* some examples of visualizing data with `ggplot`.

--

The slides/notes were created with
[Rmarkdown](https://rmarkdown.rstudio.com/), a framework for creating
fully reproducible documents.

--

Materials for this class are available on GitHub at
<https://github.com/ltierney/SIBS-WV-2025>.

--

[GitHub](https://github.com) is a social coding platform built around
the [Git](https://git-scm.com/) version control system.

--

Feel free to contact me if you have questions.

---
layout: false
class: center, middle

# Have a great time with the rest of your program!
